# ğŸ“ Text Summarization using Pegasus

This project implements an **abstractive text summarization pipeline** using **Googleâ€™s Pegasus model** (via Hugging Face Transformers).  
It trains and evaluates the model on the **SAMSum dataset** for dialogue summarization.

---

## ğŸš€ Features
- Uses **Pegasus model** for abstractive summarization
- **Data ingestion & preprocessing** handled via configuration-driven pipeline
- **Trainer API (Hugging Face)** for training & evaluation
- Configurable through **YAML files**
- Saves **trained model** and **tokenizer** for reuse

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ artifacts/                 # Saved models, checkpoints, and logs
â”‚   â”œâ”€â”€ data_ingestion/        # Downloaded raw dataset
â”‚   â”œâ”€â”€ data_transformation/   # Processed/tokenized dataset
â”‚   â”œâ”€â”€ model_trainer/         # Trained models and metrics
â”‚   â””â”€â”€ model_evaluation/      # Evaluation results (ROUGE scores, etc.)
â”‚
â”œâ”€â”€ config/                    # Configuration files (YAML)
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ research/                  # Jupyter notebooks for experiments
â”‚   â””â”€â”€ experiment.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ textSummarizer/
â”‚       â”œâ”€â”€ components/        # Core modules
â”‚       â”‚   â”œâ”€â”€ data_ingestion.py
â”‚       â”‚   â”œâ”€â”€ data_transformation.py
â”‚       â”‚   â”œâ”€â”€ model_trainer.py
â”‚       â”‚   â””â”€â”€ model_evaluation.py
â”‚       â”‚
â”‚       â”œâ”€â”€ config/            # Configuration manager
â”‚       â”‚   â””â”€â”€ configuration.py
â”‚       â”‚
â”‚       â”œâ”€â”€ constants/         # Constants used in the project
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”‚
â”‚       â”œâ”€â”€ entity/            # Entity classes for configs
â”‚       â”‚   â””â”€â”€ config_entity.py
â”‚       â”‚
â”‚       â”œâ”€â”€ logging/           # Logging setup
â”‚       â”‚   â””â”€â”€ logger.py
â”‚       â”‚
â”‚       â”œâ”€â”€ pipeline/          # Pipelines for training, evaluation, prediction
â”‚       â”‚   â”œâ”€â”€ training_pipeline.py
â”‚       â”‚   â”œâ”€â”€ evaluation_pipeline.py
â”‚       â”‚   â””â”€â”€ prediction_pipeline.py
â”‚       â”‚
â”‚       â””â”€â”€ utils/             # Utility functions
â”‚           â””â”€â”€ common.py
â”‚
â”œâ”€â”€ app.py                     # Entry point to run pipelines
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ venv/                      # Virtual environment
```


---

## âš™ï¸ Installation
```bash
# Clone the repository
git clone https://github.com/ayush2888/Deep-Learning-for-NLP-A-Summarization-Project.git
cd text-summarization-pegasus

# Create a virtual environment
python -m venv venv
source venv/bin/activate  # On Linux/Mac
venv\Scripts\activate     # On Windows

# Install dependencies
pip install -r requirements.txt

## ğŸ“Š Dataset

We use the Samsum dataset (Dialogue Summarization):
Train: 14,732 samples
Validation: 818 samples
Test: 819 samples


ğŸ› ï¸ Training the Model

To start training, run:
python app.py
